
see:
	disk.txt (really)

	https://www.thegeekdiary.com/the-ultimate-zfs-interview-questions/?PageSpeed=noscript     # good summary / tutorial here (+I still use man pages)
	https://www.howtoforge.com/tutorial/how-to-install-and-configure-zfs-on-debian-8-jessie/  # this looks technical/ detailed
	vms: https://github.com/zfsonlinux/zfs/issues/224
		Don't store RAW files as ZFS as files. There is no benefit and it is not as fast as it could be. ZVOL is perfect for that and you can snapshot a VM at a time and you have a constant (vol)block size.
		On the other hand, zol is young and in particular zvols appear to have some genuine zfs-internal code issues (bugs) as at 20180905 - where perf not an issue, files for e.g. luks/cryptsetup/veracrypt backing are just fine.

Summary:   zpool  zfs

	# info:
	zfs list
	zfs [un]mount

	zpool list
	zpool history [poolName]

	zpool import -d /dev/disk/by-id                             # attach a drive, then see what can be used/ imported by zpool:
	zpool import -d /dev/disk/by-id name                        # import a pool

	zpool export name                                           # prepare to detach drive (write all buffers/ sync disk, and mark drive as exported)
	zpool export -f name                                        # force export (forces/tries to unmount all mounts of this drive's zfs filesystems)

	# zfs pools (disks/ disk groups), ashift option is sector size, 2^12=4096 bytes, default mountpoint is /zpool-name ;
	# ashift is not always properly detected, and severely affects performance:
	zpool create -o ashift=12 -O relatime=on -O compression=on -O -m target-mountpoint poolname device   # on Linux, ONLY use /dev/disk/by-[uu]id/... names!
	zpool create -o ashift=12 poolName /dev/disk/by-uuid/...

		# dmc "device mapper crypt vol -
	zpool create -o ashift=12 -O relatime=on -O compression=on -m target-mountpoint dmcv- /dev/disk/by-id/dm-uuid-CRYPT-LUKS1-...

	zpool set comment="..." poolName                            # set settable property after creation
	zpool get all poolName                                      # list all properties and features of pool

	zfs set relatime=on poolName                                # set pool "root filesystem" property, for default values/ inheriting in sub-filesystems, after pool creation
	zfs set compression=on poolName

	zpool create -f name /dev/disk/by-id/blah                   # force, override existing partition table etc - only use if needed and MAKE SURE you're not clobbering existing disk!
	zpool destroy name
		# if unsure if newer (above 1TiB) drive is deceptive in sector size advertisement, check it FIRST! (see disk.txt):
		lsblk -o NAME,PHY-SeC
		cat /sys/block/sda/queue/physical_block_size
		cat /sys/block/sdX/queue/physical_block_size

	# zfs filesystems - for storing files:
	zfs list
	zfs list -t all
	zfs create poolName/fs-name
	zfs create -o relatime=on -o compression=off poolName/fs-name   # compression + relatime should be inherited (i.e., should set in root zfs of this pool), but compression might be set off e.g. for youtube zfs
	zfs create -m /my/preferred/mountpoint poolName/fs-name
	zfs destroy poolName/fs-name

	# properties:
	zfs get all                                                 # may have a lot of output
	zfs get all pool/youtube                                    # view [ all | specific[,property,names] ] properties for all (if no name) or named datasets
	zfs set relatime=on pool/youtube                            # set property after creation
	zfs set mountpoint=/z/dev pool/dev

	# snashots are read only (light weight):
	zfs list -t [ snap | snapshot | all ]
	zfs list -t snapshot -o name -s name                        # use this version if you have many 100s or 1000s of snapshots and performance is an issue

	zfs snapshot [-r] pool/filesystem@20180831                  # create a snapshot (-r means recursively for all sub-datasets (filesystems, volumes))
	zfs rollback pool/filesystem@20180831                       # rollback a zfs filesystem to a previous point in time (marked by a snapshot) - destroys snapshots and bookmarks that are more recent than the named snapshot
	zfs destroy pool/filesystem@20180831

	# backups - the following is insufficiently clear, need more examples TODO:
	# create initial full backup of a zpool, all its filesystems, volumes, snapshots, properties etc
	# (remember, the root fs name is the same as the pool name):
	zfs send -Rcv fs1@snapshot | zfs receive -d fs2             # check man zfs the "-d" option discards the parent (root/ poolname) path component (default new target mount dir)

	# Example: backup source "zpis1t/..." pool (and all sub fs/vols/snaps, up to snapshot "20190920-13.32"), creating/copying source into target pool/fs "bak1t/backups/zpis1t/...":
	zfs snapshot -r zpis1t@20190920-13.32                       # create recursive snapshot of zpis1t pool/fs
	zfs create bak1t/backups                                    # create parent zfs filesystem that I want to store these backups into
	zfs send -Rcv zpis1t@20190920-13.32 | zfs receive -d bak1t/backups/zpis1t  # create full recursive backup

	# to do similar, but make the backup drive a new primary, i.e. creating/copying source into target pool/fs "bak1t/...":
	zfs send -Rcv zpis1t@20190920-13.32 | zfs receive -d bak1t/zpis1t  # send to backup drive/pool

	# See also:
	#   Make send/ receive resumable:
	#   https://unix.stackexchange.com/questions/343675/zfs-on-linux-send-receive-resume-on-poor-bad-ssh-connection
	#
	#   https://pthree.org/2012/12/20/zfs-administration-part-xiii-sending-and-receiving-filesystems/
	#   https://unix.stackexchange.com/questions/289127/zfs-send-receive-with-rolling-snapshots
	#   https://docs.oracle.com/cd/E18752_01/html/819-5461/gbchx.html
	#   https://blog.fosketts.net/2016/08/18/migrating-data-zfs-send-receive/

	zfs send pool/youtube@20180831 > /my/backup/yt.bak          # make a fs/dataset backup of pool/youtube filesystem (entire? snapshot delta??)
	zfs receive -d otherpool/youtube < /my/backup/yt.bak        # import/ create a zfs filesystem, from a "zfs send"ed backup
	zfs send tank/yt@today | zfs receive -d pool2/youtube       # all in one

	# the following comes from thegeekdiary.com (above) but I don't know that it's quite correct (it may be)
	# node02 # zfs create n2pool/testfs a                       # (create a test file-system on another system)
	# node01 # zfs send n1pool/fs1@oct2013 | ssh -e none user@node02 "zfs receive -d n2pool/testfs"
	# node01 # zfs send -i @sept2013 n1pool/fs1@oct2013 | ssh -e none user@node02 zfs recv n2pool/testfs    # send only incremental data

	# local tested send/receive:
	zfs send -vRwI primary/krypt@20190914 primary/krypt@20190920 | zfs receive -vduF -o canmount=noauto bak1t/primary

	# clones are just zfs filesystems or "read-write snapshots" (made from snapshots):
	zfs clone pool/fs1@snapName pool/fs1/cloneName              # create clone as sub-filesystem
	zfs clone pool/fs1@snapName pool/cloneName                  # create clone as separate filesystem
	zfs destroy pool/cloneName                                  # delete the clone

	# bookmarks - lighter weight that snapshots, if you regularly do backups AND have a lot of file churn (created + deleted files):
	https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSBookmarksWhatFor
	http://list.zfsonlinux.org/pipermail/zfs-discuss/2017-January/027104.html
	zpool set feature@bookmarks pool                            # first enable the feature
	zfs bookmark pool/fs1@snapName                              # make a snapshot - wrong I think
	zfs snapshot pool/fs1@snapName                              # make a snapshot - correct I think
	zfs bookmark pool/fs1@snapName pool/fs1#bookmarkName        # make a bookmark
	zfs send -i #bookmarkName pool/fs1#bookmarkName ...         # can be used for incremental send
	zfs destroy pool/fs1#bookmarkName


	# Features and compression: http://open-zfs.org/wiki/Performance_tuning#Compression
	check we are using lz4 compression feature (default in newer zfs/zpools as at 20180831):
	zpool get feature@lz4_compress pool
	NAME    PROPERTY              VALUE   SOURCE
	pool    feature@lz4_compress  active  local

	should show feature@lz4_compress for each pool. If not, if old pool, then upgrade to get that that feature (or activate the feature:
	zpool upgrade ...

	# fix disconnected/reconnected USB drive:
	# primary consideration is to CREATE POOLS using /dev/disk/by-id/... names, NOT /dev/sdX names!
	# SO, if you HAVE used /dev/sdX :
	# when drive is disconnected, try this (before reconnecting):
	zpool clear poolname
	# next, try to reconnect the drive in a way which causes it to get the same /dev/sdX name
	# try adding/ removing other devices, to try to get the old /dev/sd name back for the zpool USB drive at issue
	# finally, run this once or twice again, after reconnecting:
	zpool clear poolname # sometimes, may have to run this twice
	# FIXING the dev name(s) - do this AFTER getting the pool running again (by reboot or replugging and zpool clear):
	zpool export pool
	zpool import -d /dev/disk/by-id/ pool
	zpool import -d /dev/disk/by-uuid/ pool




*



*
A zfs filesystem is also called a dataset (especially in the zfs man page).

Show the mountpoint for the root zfs filesystem in a pool that is imported and root filesystem mounted:
zfs list | grep fs_name
zfs get -o mountpoint fs_name
zfs get -Ho mountpoint fs_name
zfs get -Hro mountpoint fs_name

Change the default mountpoint for a zpool's root filesystem:
zfs set mountpoint=/new/mount/point fs_name
zfs set mountpoint=/new/mount/point zpool_name
# note that fs_name is the same as zpool_name, for the root dataset/ root filesystem on that zfs pool

# or same, for a sub filesystem/ volume:
zfs set mountpoint=/new/mount/point fs_name/fs2_name



*
copy/duplicate a zfs volume/filesystem, from one pool to another, and monitor the transfer (assumes ~85G to be transferred, modify to suit of course):
zfs send zpih2t/z/vms | pv -petars 85G | zfs receive zpis1t/z/vms

